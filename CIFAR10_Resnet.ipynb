{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b49e0149",
   "metadata": {},
   "source": [
    "# MNIST Example in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb666244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f559c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, \n",
    "                               kernel_size=3, stride=stride, \n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, \n",
    "                               kernel_size=3, stride=1, \n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3c3a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetExample(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNetExample, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)  # CIFAR-10 adjustment\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Define multiple layers with increasing channels\n",
    "        self.layer1 = self._make_layer(64, num_blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, num_blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, num_blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, num_blocks=2, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        downsample = None\n",
    "        layers = []\n",
    "        \n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        layers.append(ResidualBlock(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResidualBlock(self.in_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179f4c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(batch_size=128, num_workers=2):\n",
    "    # CIFAR-10 statistics\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2470, 0.2435, 0.2616)\n",
    "    \n",
    "    # Transformations for training and testing\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    \n",
    "    # Download and create datasets\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root='/home/shiyao/CNN_MNIST/data/CIFAR10', train=True,\n",
    "                                                 download=True, transform=transform_train)\n",
    "    \n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='/home/shiyao/CNN_MNIST/data/CIFAR10', train=False,\n",
    "                                                download=True, transform=transform_test)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=num_workers)\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                             shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "    # Hyperparameters\n",
    "batch_size = 128\n",
    "num_workers = 2\n",
    "\n",
    "# Prepare data\n",
    "train_loader, test_loader = prepare_data(batch_size=batch_size, num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11533f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        correct += calculate_accuracy(outputs, targets)\n",
    "        total += targets.size(0)\n",
    "        \n",
    "        if (batch_idx + 1) % 100 == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            print(f'Epoch [{epoch}], Step [{batch_idx+1}/{len(train_loader)}], '\n",
    "                  f'Loss: {loss.item():.4f}')\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'Train Epoch: {epoch} \\tLoss: {epoch_loss:.4f} \\tAccuracy: {epoch_acc:.2f}% \\tTime: {elapsed:.2f}s')\n",
    "\n",
    "def evaluate(model, device, test_loader, criterion, phase='Test'):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct += calculate_accuracy(outputs, targets)\n",
    "            total += targets.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    print(f'{phase} \\tLoss: {epoch_loss:.4f} \\tAccuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b302cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "ResNetExample(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1], Step [100/391], Loss: 1.9014\n",
      "Epoch [1], Step [200/391], Loss: 1.7396\n",
      "Epoch [1], Step [300/391], Loss: 1.7359\n",
      "Epoch [1], Step [391/391], Loss: 1.5743\n",
      "Train Epoch: 1 \tLoss: 1.9775 \tAccuracy: 29.33% \tTime: 16.51s\n",
      "Test \tLoss: 1.5783 \tAccuracy: 41.11%\n",
      "Best model saved with accuracy: 41.11%\n",
      "Epoch [2], Step [100/391], Loss: 1.6019\n",
      "Epoch [2], Step [200/391], Loss: 1.4878\n",
      "Epoch [2], Step [300/391], Loss: 1.2770\n",
      "Epoch [2], Step [391/391], Loss: 1.3039\n",
      "Train Epoch: 2 \tLoss: 1.4703 \tAccuracy: 45.96% \tTime: 16.72s\n",
      "Test \tLoss: 1.4402 \tAccuracy: 48.19%\n",
      "Best model saved with accuracy: 48.19%\n",
      "Epoch [3], Step [100/391], Loss: 1.3702\n",
      "Epoch [3], Step [200/391], Loss: 1.2438\n",
      "Epoch [3], Step [300/391], Loss: 1.1038\n",
      "Epoch [3], Step [391/391], Loss: 1.2365\n",
      "Train Epoch: 3 \tLoss: 1.2055 \tAccuracy: 56.67% \tTime: 16.91s\n",
      "Test \tLoss: 1.3254 \tAccuracy: 55.08%\n",
      "Best model saved with accuracy: 55.08%\n",
      "Epoch [4], Step [100/391], Loss: 0.8280\n",
      "Epoch [4], Step [200/391], Loss: 1.0024\n",
      "Epoch [4], Step [300/391], Loss: 1.1283\n",
      "Epoch [4], Step [391/391], Loss: 1.0834\n",
      "Train Epoch: 4 \tLoss: 0.9959 \tAccuracy: 64.64% \tTime: 17.02s\n",
      "Test \tLoss: 1.0660 \tAccuracy: 62.92%\n",
      "Best model saved with accuracy: 62.92%\n",
      "Epoch [5], Step [100/391], Loss: 1.0084\n",
      "Epoch [5], Step [200/391], Loss: 0.9351\n",
      "Epoch [5], Step [300/391], Loss: 1.1344\n",
      "Epoch [5], Step [391/391], Loss: 0.7484\n",
      "Train Epoch: 5 \tLoss: 0.8622 \tAccuracy: 69.61% \tTime: 17.03s\n",
      "Test \tLoss: 0.9771 \tAccuracy: 66.42%\n",
      "Best model saved with accuracy: 66.42%\n",
      "Epoch [6], Step [100/391], Loss: 0.8601\n",
      "Epoch [6], Step [200/391], Loss: 0.6395\n",
      "Epoch [6], Step [300/391], Loss: 0.7088\n",
      "Epoch [6], Step [391/391], Loss: 0.7951\n",
      "Train Epoch: 6 \tLoss: 0.7414 \tAccuracy: 74.01% \tTime: 16.99s\n",
      "Test \tLoss: 0.9765 \tAccuracy: 69.18%\n",
      "Best model saved with accuracy: 69.18%\n",
      "Epoch [7], Step [100/391], Loss: 0.6826\n",
      "Epoch [7], Step [200/391], Loss: 0.6034\n",
      "Epoch [7], Step [300/391], Loss: 0.5584\n",
      "Epoch [7], Step [391/391], Loss: 0.7798\n",
      "Train Epoch: 7 \tLoss: 0.6457 \tAccuracy: 77.55% \tTime: 17.01s\n",
      "Test \tLoss: 0.6629 \tAccuracy: 77.27%\n",
      "Best model saved with accuracy: 77.27%\n",
      "Epoch [8], Step [100/391], Loss: 0.5668\n",
      "Epoch [8], Step [200/391], Loss: 0.5372\n",
      "Epoch [8], Step [300/391], Loss: 0.5442\n",
      "Epoch [8], Step [391/391], Loss: 0.5556\n",
      "Train Epoch: 8 \tLoss: 0.5978 \tAccuracy: 79.19% \tTime: 17.07s\n",
      "Test \tLoss: 0.7249 \tAccuracy: 75.81%\n",
      "Epoch [9], Step [100/391], Loss: 0.4962\n",
      "Epoch [9], Step [200/391], Loss: 0.4707\n",
      "Epoch [9], Step [300/391], Loss: 0.5099\n",
      "Epoch [9], Step [391/391], Loss: 0.5927\n",
      "Train Epoch: 9 \tLoss: 0.5486 \tAccuracy: 81.12% \tTime: 17.06s\n",
      "Test \tLoss: 0.6789 \tAccuracy: 77.39%\n",
      "Best model saved with accuracy: 77.39%\n",
      "Epoch [10], Step [100/391], Loss: 0.6605\n",
      "Epoch [10], Step [200/391], Loss: 0.5512\n",
      "Epoch [10], Step [300/391], Loss: 0.5332\n",
      "Epoch [10], Step [391/391], Loss: 0.5208\n",
      "Train Epoch: 10 \tLoss: 0.5248 \tAccuracy: 81.89% \tTime: 17.01s\n",
      "Test \tLoss: 0.7714 \tAccuracy: 74.94%\n",
      "Epoch [11], Step [100/391], Loss: 0.4349\n",
      "Epoch [11], Step [200/391], Loss: 0.3171\n",
      "Epoch [11], Step [300/391], Loss: 0.3137\n",
      "Epoch [11], Step [391/391], Loss: 0.2260\n",
      "Train Epoch: 11 \tLoss: 0.3513 \tAccuracy: 88.01% \tTime: 17.03s\n",
      "Test \tLoss: 0.3544 \tAccuracy: 87.79%\n",
      "Best model saved with accuracy: 87.79%\n",
      "Epoch [12], Step [100/391], Loss: 0.2801\n",
      "Epoch [12], Step [200/391], Loss: 0.3050\n",
      "Epoch [12], Step [300/391], Loss: 0.2798\n",
      "Epoch [12], Step [391/391], Loss: 0.1651\n",
      "Train Epoch: 12 \tLoss: 0.2930 \tAccuracy: 90.01% \tTime: 17.04s\n",
      "Test \tLoss: 0.3346 \tAccuracy: 88.50%\n",
      "Best model saved with accuracy: 88.50%\n",
      "Epoch [13], Step [100/391], Loss: 0.2131\n",
      "Epoch [13], Step [200/391], Loss: 0.2921\n",
      "Epoch [13], Step [300/391], Loss: 0.1997\n",
      "Epoch [13], Step [391/391], Loss: 0.2837\n",
      "Train Epoch: 13 \tLoss: 0.2679 \tAccuracy: 90.74% \tTime: 17.06s\n",
      "Test \tLoss: 0.3319 \tAccuracy: 88.85%\n",
      "Best model saved with accuracy: 88.85%\n",
      "Epoch [14], Step [100/391], Loss: 0.2505\n",
      "Epoch [14], Step [200/391], Loss: 0.3225\n",
      "Epoch [14], Step [300/391], Loss: 0.1737\n",
      "Epoch [14], Step [391/391], Loss: 0.3459\n",
      "Train Epoch: 14 \tLoss: 0.2506 \tAccuracy: 91.41% \tTime: 17.01s\n",
      "Test \tLoss: 0.3258 \tAccuracy: 88.91%\n",
      "Best model saved with accuracy: 88.91%\n",
      "Epoch [15], Step [100/391], Loss: 0.1780\n",
      "Epoch [15], Step [200/391], Loss: 0.2045\n",
      "Epoch [15], Step [300/391], Loss: 0.1525\n",
      "Epoch [15], Step [391/391], Loss: 0.2009\n",
      "Train Epoch: 15 \tLoss: 0.2390 \tAccuracy: 91.85% \tTime: 17.04s\n",
      "Test \tLoss: 0.3143 \tAccuracy: 89.29%\n",
      "Best model saved with accuracy: 89.29%\n",
      "Epoch [16], Step [100/391], Loss: 0.2451\n",
      "Epoch [16], Step [200/391], Loss: 0.1533\n",
      "Epoch [16], Step [300/391], Loss: 0.2108\n",
      "Epoch [16], Step [391/391], Loss: 0.1725\n",
      "Train Epoch: 16 \tLoss: 0.2024 \tAccuracy: 93.14% \tTime: 17.05s\n",
      "Test \tLoss: 0.2903 \tAccuracy: 90.21%\n",
      "Best model saved with accuracy: 90.21%\n",
      "Epoch [17], Step [100/391], Loss: 0.1500\n",
      "Epoch [17], Step [200/391], Loss: 0.1162\n",
      "Epoch [17], Step [300/391], Loss: 0.1866\n",
      "Epoch [17], Step [391/391], Loss: 0.2551\n",
      "Train Epoch: 17 \tLoss: 0.1899 \tAccuracy: 93.66% \tTime: 17.09s\n",
      "Test \tLoss: 0.2884 \tAccuracy: 90.33%\n",
      "Best model saved with accuracy: 90.33%\n",
      "Epoch [18], Step [100/391], Loss: 0.2711\n",
      "Epoch [18], Step [200/391], Loss: 0.1006\n",
      "Epoch [18], Step [300/391], Loss: 0.1415\n",
      "Epoch [18], Step [391/391], Loss: 0.3694\n",
      "Train Epoch: 18 \tLoss: 0.1845 \tAccuracy: 93.66% \tTime: 17.07s\n",
      "Test \tLoss: 0.2878 \tAccuracy: 90.14%\n",
      "Epoch [19], Step [100/391], Loss: 0.1540\n",
      "Epoch [19], Step [200/391], Loss: 0.1614\n",
      "Epoch [19], Step [300/391], Loss: 0.0969\n",
      "Epoch [19], Step [391/391], Loss: 0.2912\n",
      "Train Epoch: 19 \tLoss: 0.1845 \tAccuracy: 93.81% \tTime: 17.03s\n",
      "Test \tLoss: 0.2882 \tAccuracy: 90.20%\n",
      "Epoch [20], Step [100/391], Loss: 0.1263\n",
      "Epoch [20], Step [200/391], Loss: 0.2132\n",
      "Epoch [20], Step [300/391], Loss: 0.1674\n",
      "Epoch [20], Step [391/391], Loss: 0.1512\n",
      "Train Epoch: 20 \tLoss: 0.1814 \tAccuracy: 93.79% \tTime: 17.06s\n",
      "Test \tLoss: 0.2849 \tAccuracy: 90.35%\n",
      "Best model saved with accuracy: 90.35%\n",
      "Final Test Accuracy: 90.35%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    num_epochs = 20\n",
    "    batch_size = 128\n",
    "    learning_rate = 0.1\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    num_workers = 2\n",
    "    milestones = [10, 15]\n",
    "    gamma = 0.1\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # Prepare data\n",
    "    train_loader, test_loader = prepare_data(batch_size=batch_size, num_workers=num_workers)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ResNetExample(num_classes=10).to(device)\n",
    "    print(model)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum, weight_decay=weight_decay)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=gamma)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train(model, device, train_loader, criterion, optimizer, epoch)\n",
    "        test_loss, test_acc = evaluate(model, device, test_loader, criterion)\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save the best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'best_resnet_cifar10.pth')\n",
    "            print(f'Best model saved with accuracy: {best_acc:.2f}%')\n",
    "    \n",
    "    print(f'Final Test Accuracy: {best_acc:.2f}%')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
